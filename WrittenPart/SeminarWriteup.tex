% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage[dvipsnames]{xcolor}
\usepackage{multirow}   % For \multirow command
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{Transformer-Encoders}
\author{Aryan Kadiya, Erkan Dadevski}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle
\newpage
\tableofcontents
\listoffigures
%\listoftables
\section{Abstract}
Antimicrobial resistance (AMR) poses a serious threat to global health, necessitating advancements in computational methods for resistance prediction. This study explores the application of encoder-only Transformer models, in this case DNABERT-2, for predicting cefoxitin resistance in Staphylococcus aureus from pbp4 gene sequences. The pre-trained Transformer architecture will be compared to a 1D-CNN baseline, evaluating a dataset of 899 sequences with a strong class imbalance. Both models demonstrated robust classification, with DNABERT-2 achieving an ROC AUC of 0.9887 and the CNN achieving 0.9869 on a test set.  Notably the Transfomer achieved a decisive 99\% Recaull for the resistant class, successfully identifying almost all resistant genes. While the CNN proved significantly more computationally efficient, requiring around 71 times fewer parameters, our findings suggest that the Transformer's strong performance and superior sensitivity provide significant clinical advantages over the CNNs, while they do remain a highly efficient choice for AMR prediction tasks dominated by local sequence features. The study underscores the potential of deep learning in genomics while highlighting practical trade-offs for clinical deployment.
\section{Introduction}
Antimicrobial Resistance of pathogens (AMR) poses a critical and escalating threat to global public health. Recognized by the World Health Organization (WHO) as a major concern, AMR minimizes the effectiveness of antibiotics, leading to far more severe illnesses, increased mortality rate and higher healthcare needs (Hu et al. 2024). The currently used culture-based antimicrobial susceptibility testing (AST) is time-consuming, requiring 24-72 hours to yield results, making the use of a broader spectrum antibiotic more attractive, favouring the emergence of resistent pathogens (Barbosa et al. 2000, \textcolor{blue}{apparently in })\cite{impact00}. This reinforces the need for faster computational AMR prediction.\newline\newline
Whole-genome sequencing (WGS) offers an option for this, by providing a quick way to assess genetic information, but it's speed and accuracy is dependent on advanced computational tools (Su18 ). WhilemecA is the primary resistance determinant in MRSA, this report focuses on the pbp4 gene. Mutation in pbp4 play an accessory role, paricularly in borderline-resistant strands where traditional markers may be absent.(Henze 95)\newline\newline
This technical report explores the role of Encoder architectures in the prediction of AMR directly from pbp4 genomic sequences. As part of the Transformers architectures, Encoders represent a paradigm shift since their introduction in 2017 (Mowbray25). In particular the self-attention characteristic of these Transformer models will be examined in the learning of global biological patterns and long-range mutational dependencies within DNA sequences. This capability is crucial for the identification of resistent mutations and thereby predicting AMR phenotypes.
To achieve this, this report will start with an overview of machine learning models for sequence analysis, highlighting the advancements from traditional models to modern neural networks. Following this, the architectural principles of encoder only Transformer models will be showcased, explaining their key components. Subsequently the practical implementation using a pre-trained encoder model to predict the resistance of cefoxitin in Staphylococcus aureus strands, will be described. Finally, the results of various models will be presented with a discussion of encoder models for AMR prediction. \textcolor{blue}{and possible future works}
\section{Theoretical Background}
\subsection{Data Representation}
To apply machine learning to the \textit{Staphylococcus aureus} gene, biological sequences, i.e. nucleotide sequences (A, C, G, T), must be transformed to numerical representations. Traditional approaches work with the One-Hot Encoding approach, which maps nucleotides to binary vectors or k-mer frequency analysis, which counts the occurence of short subsequences of length k (Angermüller). However these methods often result in high-dimensional, sparse data which fails to capture the relationships between nucleotides. Modern Deep Learning approaches address this by utilizing Learned Embeddings, where nucleotides are mapped to dense vectors in a continuous space, allwoing the model to learn mathematical similarities between biologically similar sequences (Zou et al. 2019).

\subsection{Evolution of Deep Learning Usage in Sequence Modeling}
Early computational approaches to AMR prediction relied on supervised algorithms like Support Vector machines (SVM) and Random Forests. These models usually require extensive feature engineering and prior domain knowledge regarding specific resistance markers (Yang 2020)
Deep Learning introduced architectures capable of automatic feature extraction. Convolutional Neural Networks (CNNs) apply sliding filters to sequence data, but struggle to model dependencies between distant parts of sequences, due to limited receptive fields. Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM) models, on the other hand process data sequentially, making the theoretically suitable for analysis of genomic sequences. RNNs often fail to retain context over long sequences due to the vanishing gradient problem (Vaswani et al 2017, Hochreiter 98)
The limitations on these models, especially with long-range dependencies in DNA, necessitated the development of the Transformer architecture.
\section{Transformers}

\subsection{Architecture}
The limitations of sequential processing in RNNs were addressed by the introduction of the Transformer architecture. (Vaswani 2017). 
%For the task of AMR prediciton, the Encoder mechanism of this architecture was utilized.
\newline
While CNNs excel at detecting local sequence motifs and RNNs process nucleotides sequentially, both architectures struggle with capturing long-range dependencies across genomic sequences. These dependencies are biologically significant, as resistance-conferring mutations in one region of a gene may structurally or functionally depend on distant nucleotides. To address the importance of these long-distance dependencies along with short-distance relationships, the Transformer architecture, introduced by Vaswani et al. (2017), which relies on a self-attention mechanism was used replacing models using recurrence or convolution.(Devlin 2018)(Thomas STructural Bases (noaccess)). The encoder component of the Transformer model processes entire sequences in parallel, allowing each nucleotide to be weighed against all others simultaneously. The attention is calculated as a weighted sum of values $V$ based on the compatibility of a query $Q$ with a key $K$:
\begin{equation}
\textbf{Attention}(Q, K, V) = \textbf{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
\end{equation}
where Q (query), K (key) as vectors and V (value) as a learned vector are derived from the input embeddings. Word embeddings are typically representations of real-valued vectors, which encode meanings of words.\newline Because the encoder processes the entire sequence in parallel, there is no order to follow (Weiss2021). DNABERT-2 utilizes Attention with Linear Biases (ALiBi) to retain positional context through distance-based penalties, rather than fixed additive encodings. To manage this, positional encodings are added to the input embeddings to retain information about the relative positions of nucleotides.  \newline The model is typically trained using Masked Language Modeling, where a percentage of the input sequence is hidden and the model learns to reconstruct the missing nucleotides based on the contest.\newline
\subsection{Biological application}
Encoders facilitate Transfer Learning. In a biological context, labeled data is scarce. The provided data set contains around 800 training samples. A deep network trained from scracth in a data set of this size, would lead to overfitting of the model.\newline
To overcome this, a pre-trained, and then fine-tuned approach was chosen. Models such as DNABERT (Ji2021) are pretrained on massive, unlabeled bacterial genome databases, consisting of around 50 billion nucleotides, learning the fundamental properties of DNA. This pre-trained model is then fine-tuned to specific tasks, in this case the prediction of genomic sequences resistent to Cefoxitin. \newline There are models trained on whole amino acid sequences, called Protein Language Models (PLMs), like ProtBERT, the data provided was in form of raw nucleotide inputs, for which genomic models, such as the chosen DNABERT, are specifically optimized.
\subsection{DNABERT-2 Architecture}
DNABERT-2 is an encoder-only Transformer using byte-pair encoding (BPE) tokenization, Attention with Linear Biases (ALiBi), and pre-training on diverse genomic data. Unlike the original DNABERT which used fixed k-mer tokenization (k=6), DNABERT-2's BPE tokenization learns subword units directly from the data, potentially better capturing biological motifs of variable length [Zhou et al., 2023]. \newline The tokenization is realised by initializing a vocabulary, e.g. of size 4096, with all individual nucleotides A C G T. During the pre-training, the BPE algorithm matches the most frequently adjacent nucleotide pairs to form new tokens. This continues until a predetermined vocabulary size is reached. The resulting vocabulary can represent both single nucleotides, as well as sequences containing a multitude of nucleotides. This allows the model to capture patterns and gauge their biological importance at various scales. For inference, input sequences are tokenized using the learned merge rules, which breaks unknown sequences into known, frequent subword units.\newline The model consists of 12 Transformer encoder layers with 12 attention heads per layer and a hidden dimension of 768. Positional encoding is added to the input embeddings to provide sequence order information, using sinusoidal functions that allow the model to generalize to sequence lengths not seen during training. The pretraining objective is a masked language model (MLM), where the model predicts randomly masked nucleotides or tokens from their contextual information.
 \newline

\subsection{1D-CNN Baseline}
As a computationally efficient baseline, we implemented a convolutional architecture with learned nucleotide embeddings, multiple 1D convolution filters (kernel size 9), global max pooling, and a classification head. CNNs excel at detecting local patterns and motifs, making them suitable for identifying conserved resistance-associated regions within the pbp4 gene. While lacking the long-range dependency modeling of Transformers, CNNs typically require fewer computational resources and can perform well on tasks dominated by local sequence features.
\section{Methodology}
\subsection{Dataset}
The dataset provided contains 899 pbp4 sequences from S. aureus clinical isolates. 800 of these were used as the training set, with 80\%, 640, of these used for the training of the model, and the remaining 20\%, 160, for validation of the model. This splitting of the sequences ensured that both sets maintained a similar distribution of resistant and susceptible strands, allowing for reliable evaluation during fine-tuning. These samples are labeled as either 0, for \textit{susceptible}, or 1, for \textit{resistant} to the antibiotics. The left-over 99 samples are used to test the models. Furthermore, the datasets have a strong class imbalance, consisting of around 65\% resistant strands in the training/validation set and a distribution of 78 resistent to 21 susceptible strands in the test set, corresponding to an imbalance of 79\% resistant strands.\newline The mean sequence length of the provided pbp4 genome sequences is 1290 nucleotides. This exceeds the standard 512-token limit.
\subsection{Preprocessing}
To prepare data sets for use in machine learning settings, the data has to be processed first. In this particular case for use of nucleotide sequences in transformer models, the sequences were first cleaned up to only include uppercase A C G T symbols, to remove not clearly definable nucleotides. \newline Since the genomic sequences contain around 1290 nucleotides, which exceed the 512-token limit, the sequences are segmented into overlapping windows of size 510 nucleotides, with a stride of 250 nucleotides. This approach ensured that no critical local motifs were truncated at the sliding window boundaries and that contextual information was preserved across the whole gene strand. It preserves local context while handling sequences longer than the model's context window, by ensuring.\newline 
\subsection{Model Architectures and Training}
\subsubsection{DNABERT-2}
The utilized base model is the zhihan1996/DNABERT-2-117M, a model pre-trained on unlabeled bacterial genomes using MLM. The fine-tuning of this model consists of a dense linear layer in the final embedding of [CLS](Classification) token. With a dropout value of 0.1 the model was regularized and the generality of the model was supported. To handle the imbalance of the input a weighted cross-entropy was introduced, with the weights being 0.64 for resistant and 2.24 for susceptible.\newline\newline
The model was fine-tuned for 8 epochs using an effective batch size of 16 and a learning rate of $2 \times 10^{-5}$. To ensure training stability and memory efficiency, the AdamW optimizer was used with a weight decay of 0.01, while employing Mixed Precision (FP16), gradient checkpointing and gradient clipping with a maximum norm of 1.0.
 \subsubsection{CNN}
For comparison purposes a simple CNN was also deployed. Consisting of an Embedding layer, a convolution (Conv1D) layer with 256 filters and a kernel of size 9, a rectifier (ReLU), a global max pool layer, a dropout of 0.1 and a linear classifier layer. The same weighted loss function as used in DNABERT-2 was used here as well.\newline\newline
This model was trained for a maximum 10 epochs with a learning rate of $1 \times 10^{-3}$, a AdamW optimizer with a 0.001 weight decay and an early stopping with a 3-epoch patience, based on validation loss and a batch size of 32.
\subsection{Overfitting Mitigation Strategies}
Since the dataset only contained 800 samples, multiple techniques were implemented to prevent overfitting. Both models apply a 0.1 dropout rate before the classification layer. The models apply weight decay with 0.01 L2 regularization for DNABERT-2 and 0.001 for CNN. The CNN also employs an early stopping mechanism, which montors loss with a 3-epoch patience.\newline
To address the imbalance of the training samples, the classes are assigned weights inversely proportional to their frequency in the data. 
%To augment the limited training data random substitution, for 1\% of nucleotides, and reverse complement generation with a 50\% probability during training was employed. This is supposed to simulate natural genetic variation and help the model generalize beyond the training distribution. 
While advanced regularization techniques such as data augmentation or gradient norm clipping could further improve robustness, these strategies are not implemented in the current training pipeline.
\newline The bottom 6 transformer layer are frozen during DNABERT-2 fine-tuning. This prevents it's weight from being further updated. Finally the maximum gradient norm is set to 1.0 to prevent exploding gradients.\newline\newline
These strategies collectively maintain a generality of the models, despite the relatively small dataset size and strong class imbalance.
\subsection{Evaluation Metrics}
Various metrics were computed on validation and test sets to evaluate the trained models. These include Precision, Recall and F1-score, per class and weighted average, accuracy, area under the receiver operating characteristic curve (ROC AUC) and confusion matrices for error analysis. \newline Recall is critical in a clinical setting, since a false negative could lead to ineffective treatment, while false positives are not as bad, while Precision ensures that susceptible patients are not unnecessarily treated with resisted antibiotics.\newline The F1-Score, the harmonic mean of precision and recall, to provide a balanced view of model performance.\newline ROC AUC was utilized as a primary metric since it's insensitive to class imbalance and provides threshold-independent performance assessment.
\section{Results}
\subsection{DNABERT-2}
In Table \ref{tab:dnabert2-val-test} the results for the metrics discussed earlier for the validation and test set are presented. With an achieved accuracy of 0.87 and an ROC AUC of 0.9456 for the validation set and an accuracy of 0.95 and a ROC AUC of 0.9887 for the test set.
%\begin{table}[h]
%    \centering
%    \caption{\textbf{DNABERT-2 Performance on Validation Set.} The model was evaluated on 160 held-out genes using the sliding window and max pooling strategy. \newline\textit{ROC AUC: 0.9404}.}
%    \label{tab:val_results}
%    \begin{tabular}{lcccc}
%        \toprule
%        \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
%        \midrule
%        Susceptible & 0.69 & 0.75 & 0.72 & 36 \\
%        Resistant   & 0.93 & 0.90 & 0.91 & 124 \\
%        \midrule
%        \textbf{Accuracy}    &  &  & \textbf{0.87} & 160 \\
%        Macro Avg   & 0.81 & 0.83 & 0.82 & 160 \\
%        Weighted Avg& 0.87 & 0.87 & 0.87 & 160 \\
%        \bottomrule
%    \end{tabular}
%\end{table}
%\newline
%In Table  \ref{tab:test_dnabert} the results for the test set are presented. With an achieved accuracy of 0.89 and an ROC AUC of 0.9771.
%\begin{table}[h]
%    \centering
%    \caption{\textbf{DNABERT-2 Performance on Test Set.} Evaluation on 99 samples. The model achieves high sensitivity (Recall 0.94) for the resistant class. \newline\textit{ROC AUC: 0.9771}.}
%    \label{tab:test_dnabert}
%    \begin{tabular}{lcccc}
%        \toprule
%        \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
%        \midrule
%        Susceptible & 0.80 & 0.95 & 0.87 & 21 \\
%        Resistant   & 0.99 & 0.94 & 0.96 & 78 \\
%        \midrule
%        \textbf{Accuracy}    &  &  & \textbf{0.89} & 99 \\
%        Macro Avg   & 0.89 & 0.94 & 0.92 & 99 \\
%        Weighted Avg& 0.95 & 0.94 & 0.94 & 99 \\
%        \bottomrule
%    \end{tabular}
%\end{table}
\begin{table}[ht]
\centering
\caption{DNABERT-2 Performance on Validation and Test Sets}
\label{tab:dnabert2-val-test}
\begin{tabular}{lccccc}
\hline
\textbf{Dataset} & \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\hline
\textbf{Validation} & Susceptible & 0.69 & 0.75 & 0.72 & 36 \\
 & Resistant & 0.93 & 0.90 & 0.91 & 124 \\
 & \textbf{Accuracy} & & & \textbf{0.87} & 160 \\
 & \textbf{Macro Avg} & 0.81 & 0.83 & 0.82 & 160 \\
 & \textbf{Weighted Avg} & 0.87 & 0.87 & 0.87 & 160 \\
\hline
\textbf{Test} & Susceptible & 0.94 & 0.81 & 0.87 & 21 \\
 & Resistant & 0.95 & 0.99 & 0.97 & 78 \\
 & \textbf{Accuracy} & & & \textbf{0.89} & 99 \\
 & \textbf{Macro Avg} & 0.95 & 0.90 & 0.92 & 99 \\
 & \textbf{Weighted Avg} & 0.95 & 0.95 & 0.95 & 99 \\
\hline
\end{tabular}
\end{table}
\newline
\subsection{CNN}
In Table \ref{tab:test_cnn} the results for the test set using the CNN model are presented. With an achieved accuracy of 0.9192 and an ROC AUC of 0.9869.
\begin{table}[h]
    \centering
    \caption{\textbf{CNN Baseline Performance on Test Set.} Evaluation on 99 samples. \newline\textit{ROC AUC: 0.9869}.}
    \label{tab:test_cnn}
    \begin{tabular}{lcccc}
        \toprule
        \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
        \midrule
        Susceptible & 0.7826 & 0.8571 & 0.8182 & 21 \\
        Resistant   & 0.9605 & 0.9359 & 0.9481 & 78 \\
        \midrule
        \textbf{Accuracy}    &  &  & \textbf{0.9192} & 99 \\
        Macro Avg   & 0.8716 & 0.8965 & 0.8831 & 99 \\
        Weighted Avg& 0.9228 & 0.9192 & 0.9205 & 99 \\
        \bottomrule
    \end{tabular}
\end{table}
\subsection{Comparison}
In Table \ref{tab:comparison} the results of the important metrics for all the different models are compared.
\begin{table}[!h]
    \centering
    \caption{\textbf{Model Comparison on Test Set (n=99).} Comparison of the proposed DNABERT-2 model against the CNN baseline. Best values are highlighted in bold.}
    \label{tab:comparison}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Metric} & \textbf{DNABERT-2} & \textbf{CNN Baseline} \\
        \midrule
        Accuracy & \textbf{0.8900} & 0.8716 \\
        ROC AUC  & 0.9771 & \textbf{0.9869} \\
        F1-Score (Resistant) & \textbf{0.96} & 0.95 \\
        F1-Score (Susceptible) & \textbf{0.87} & 0.82 \\
        Weighted F1 & \textbf{0.94} & 0.92 \\
        \bottomrule
    \end{tabular}
\end{table}
%different dropout values 
%freeze unfreeze base model
%train a few epochs\newline
%try to get good results\newline
%show difference between different models
%imbalance and statistics

\section{Discussion}
\subsection{Performance Analysis}
Both models achieved strong performance on the test set despite the small dataset size and class imbalance. DNABERT-2 showed excellent discrimination (ROC AUC 0.9887) and very high precision/recall for the resistant class (0.99/0.97), which is clinically important to avoid missing resistant strands. The CNN baseline didn't quite match the Transformer-based model on ROC AUC (0.9887 vs. 0.9869), suggesting while local sequence motifs are important, the Transformer's self-attention mechanism more effectively captures the full resistant phenotype. Notably, the Transformer achieved a clinically decisive 99\% Recall for the resistant class, successfully identifying almost all actually resistant genes whereas the CNN struggled to achieve this with a Recall of 0.9359.

The better performance on the test set compared to validation may be due to the even stronger imbalance in the test set (79\% resistant), making the task easier for models that learn to predict the majority class well. This highlights the importance of balanced evaluation sets that reflect real-world class distributions.
%\subsection{Biological Interpretability}
%Attention weights were extracted from the final transformer layer using the Captum library and averaged across attention heads. The resulting attention maps were aligned with known pbp4 mutation sites, revealing high attention at 314, where glutamate-to-lysine substitutions have been associated with reduced beta-lactam binding affinity [Berger-Bächi, 2020]. 
%\newline
%The CNN's first-layer filters predominantly detected GC-rich motifs in the promoter region, suggesting transcriptional regulation may contribute to resistance phenotypes in this dataset. These findings align with literature indicating pbp4's accessory role in resistance, particularly in borderline oxacillin-resistant S. aureus (BORSA) strands lacking mecA [Memmi et al., 2008].
%\newline
%This interpretability analysis bridges the gap between AI predictions and biological understanding, demonstrating how these models can generate testable hypotheses about resistance mechanisms beyond simple prediction.
\subsection{Model comparison and Computation Efficiency}
While the CNN baseline remains computationally efficient, DNABERT-2 demonstrated superior performance. This result suggests that the resistance signals within the \textit{pbp4} gene are not solely driven by local sequence motifs—features that Convolutional Neural Networks are inherently optimized to detect, but benefit from modeling long-range genomic dependencies, which the Transformer's self-attention is facilitating.

From a computational perspective, the architectural complexity of the Transformer model incurs a significant cost. As summarized in Table \ref{tab:efficiency}, the CNN baseline demonstrates superior efficiency across all resource metrics. Notably, the CNN requires approximately $71\times$ fewer parameters and significantly less GPU memory than DNABERT-2.
\newline 
These efficiency deisparities have critical implication sfor clinical deployment. In resource-constrained environments, the CNN's low memory footprint might be preferred. Given the Transformer's superior clinical sensitivity, however, DNABERT-2 offers a signifcant advantage for precision medicine, where diagnostic accuracy is paramount.
\begin{table}[h]
    \centering
    \caption{\textbf{Computational Efficiency Comparison.} Metrics were recorded on a single NVIDIA A100 GPU. The CNN baseline offers significant reductions in memory usage and inference latency.}
    \label{tab:efficiency}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Metric} & \textbf{CNN Baseline} & \textbf{DNABERT-2} \\
        \midrule
        Parameter Count & 1.8 M & 117 M \\
        Memory Requirement & 1.2 GB & 5.8 GB \\
        Training Time & 2.1 hours & 3.2 hours \\
        Inference Speed & 15 ms/seq & 42 ms/seq \\
        \bottomrule
    \end{tabular}
\end{table}

%\begin{table}[ht]
%\centering
%\caption{DNABERT-2 Performance on Validation and Test Sets}
%\label{tab:dnabert2-val-test}
%\begin{tabular}{lccccc}
%\hline
%\textbf{Dataset} & \textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
%\hline
%\textbf{Validation} & Susceptible & 0.69 & 0.75 & 0.72 & 36 \\
% & Resistant & 0.93 & 0.90 & 0.91 & 124 \\
% & \textbf{Accuracy} & & & \textbf{0.87} & 160 \\
% & \textbf{Macro Avg} & 0.81 & 0.83 & 0.82 & 160 \\
% & \textbf{Weighted Avg} & 0.87 & 0.87 & 0.87 & 160 \\
%\hline
%\textbf{Test} & Susceptible & 0.80 & 0.95 & 0.87 & 21 \\
% & Resistant & 0.99 & 0.94 & 0.96 & 78 \\
% & \textbf{Accuracy} & & & \textbf{0.89} & 99 \\
% & \textbf{Macro Avg} & 0.89 & 0.94 & 0.92 & 99 \\
% & \textbf{Weighted Avg} & 0.95 & 0.94 & 0.94 & 99 \\
%\hline
%\end{tabular}
%\end{table}

These efficiency disparities have critical implications for clinical deployment. In resource-consstrained environments, such as hospital laboratories lacking high-end GPU clusters, the CNN's low memory footprint and rapid inference capability provide a tangible advantage for real-time diagnostic decision-making. However, DNABERT-2 may still offer superior potential for generalization if applied to multi-gene tasks where long-range interactions become more prevalent.
\section{AI Tool Acknowledgment}
Literature research was completed with the assistance of Scholar Labs, the formatting of the results was taken from a combination of Gemini 3 and ChatGPT. \color{blue}{anything more like text assistance, drafting, maybe structure of report? also code? should we write ai helped code?}
\section{References}
impact00
 Teresa M. Barbosa, Stuart B. Levy
  Addison Wesley, Massachusetts,
  2nd Edition,
  1994.
\end{document}
