@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@article{impact00,
  title={The impact of antibiotic use on resistance development and persistence},
  author={Barbosa, Teresa M and Levy, Stuart B},
  journal={Drug Resistance Updates},
  volume={3},
  number={5},
  pages={303--311},
  year={2000},
  doi={10.1054/drup.2000.0167},
  url={https://www.sciencedirect.com/science/article/pii/S1368764600901675}
}


@article{hu2024global, title={Assessing computational predictions of antimicrobial resistance phenotypes from microbial genomes}, volume={25}, DOI={10.1093/bib/bbae206}, number={3}, journal={Briefings in Bioinformatics}, author={Hu, Kaixin and Meyer, Fernando and Deng, Zhi-Luo and Asgari, Ehsaneddin and Kuo, Tzu-Hao and Münch, Philipp C and McHardy, Alice C}, year={2024}, month={Mar}} 


@article{angermueller2016deep,
  title={Deep learning for computational biology},
  author={Angermueller, Christof and P{\"a}rnamaa, Tanel and Parts, Leopold and Stegle, Oliver},
  journal={Molecular systems biology},
  volume={12},
  number={7},
  pages={878},
  year={2016}
}

@article{zou2019primer, title={A primer on Deep Learning in Genomics}, volume={51}, DOI={10.1038/s41588-018-0295-5}, number={1}, journal={Nature Genetics}, author={Zou, James and Huss, Mikael and Abid, Abubakar and Mohammadi, Pejman and Torkamani, Ali and Telenti, Amalio}, year={2018}, month={Nov}, pages={12–18}} 
@article{yang2020machine,
  title={Machine learning applications in antimicrobial resistance},
  author={Yang, Y and others},
  journal={Nature Reviews Genetics},
  year={2020}
}

@article{hochreiter1997long,
  title={Recurrent Neural Net Learning and Vanishing Gradient},
  author={Hochreiter, Sepp},
  journal={International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  volume={6},
  number={2},
  pages={107--116},
  year={1998}
}

@misc{devlin2018bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@article{ji2021dnabert,
    author = {Ji, Yanrong and Zhou, Zhihan and Liu, Han and Davuluri, Ramana V},
    title = {DNABERT: pre-trained Bidirectional Encoder Representations from Transformers model for DNA-language in genome},
    journal = {Bioinformatics},
    volume = {37},
    number = {15},
    pages = {2112-2120},
    year = {2021},
    month = {02},
    abstract = {Deciphering the language of non-coding DNA is one of the fundamental problems in genome research. Gene regulatory code is highly complex due to the existence of polysemy and distant semantic relationship, which previous informatics methods often fail to capture especially in data-scarce scenarios.To address this challenge, we developed a novel pre-trained bidirectional encoder representation, named DNABERT, to capture global and transferrable understanding of genomic DNA sequences based on up and downstream nucleotide contexts. We compared DNABERT to the most widely used programs for genome-wide regulatory elements prediction and demonstrate its ease of use, accuracy and efficiency. We show that the single pre-trained transformers model can simultaneously achieve state-of-the-art performance on prediction of promoters, splice sites and transcription factor binding sites, after easy fine-tuning using small task-specific labeled data. Further, DNABERT enables direct visualization of nucleotide-level importance and semantic relationship within input sequences for better interpretability and accurate identification of conserved sequence motifs and functional genetic variant candidates. Finally, we demonstrate that pre-trained DNABERT with human genome can even be readily applied to other organisms with exceptional performance. We anticipate that the pre-trained DNABERT model can be fined tuned to many other sequence analyses tasks.The source code, pretrained and finetuned model for DNABERT are available at GitHub (https://github.com/jerryji1993/DNABERT).Supplementary data are available at Bioinformatics online.},
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/btab083},
    url = {https://doi.org/10.1093/bioinformatics/btab083},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/37/15/2112/57195892/btab083.pdf},
}

@misc{zhou2023dnabert2,
      title={DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome}, 
      author={Zhihan Zhou and Yanrong Ji and Weijian Li and Pratik Dutta and Ramana Davuluri and Han Liu},
      year={2024},
      eprint={2306.15006},
      archivePrefix={arXiv},
      primaryClass={q-bio.GN},
      url={https://arxiv.org/abs/2306.15006}, 
}

@article{su2018computational, title={Genome-based prediction of bacterial antibiotic resistance}, volume={57}, DOI={10.1128/jcm.01405-18}, number={3}, journal={Journal of Clinical Microbiology}, author={Su, Michelle and Satola, Sarah W. and Read, Timothy D.}, year={2019}, month={Mar}} 

@article{henze1995pbp4,
  title={Pbp4 mutations in S. aureus},
  author={Henze, U and others},
  journal={Journal of Bacteriology},
  year={1995}
}
@article{weiss2021,
  author       = {Gail Weiss and
                  Yoav Goldberg and
                  Eran Yahav},
  title        = {Thinking Like Transformers},
  journal      = {CoRR},
  volume       = {abs/2106.06981},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.06981},
  eprinttype    = {arXiv},
  eprint       = {2106.06981},
  timestamp    = {Tue, 15 Jun 2021 16:35:15 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-06981.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{saureuspbp4,
author = {U U Henze and B Berger-Bächi},
title = {Staphylococcus aureus penicillin-binding protein 4 and intrinsic beta-lactam resistance},
journal = {Antimicrobial Agents and Chemotherapy},
volume = {39},
number = {11},
pages = {2415-2422},
year = {1995},
doi = {10.1128/aac.39.11.2415},

URL = {https://journals.asm.org/doi/abs/10.1128/aac.39.11.2415},
eprint = {https://journals.asm.org/doi/pdf/10.1128/aac.39.11.2415}
,
    abstract = { 
        Increased levels of production of penicillin-binding protein PBP 4 correlated with in vitro acquired intrinsic beta-lactam resistance in a mutant derived from a susceptible strain of Staphylococcus aureus, strain SG511 Berlin. Truncation of the PBP 4 C-terminal membrane anchor abolished the PBP 4 content of cell membrane preparations as well as the resistance phenotype. A single nucleotide change and a 90-nucleotide deletion, comprising a 14-nucleotide inverted repeat in the noncoding pbp4 gene promoter proximal region, were the only sequence differences between the resistant mutant and the susceptible parent. These mutations were thought to be responsible for the observed overproduction of PBP 4 in the intrinsically beta-lactam-resistant mutant. The pbp4 gene was flanked upstream by the open reading frame abcA, coding for an ATP-binding cassette transporter-like protein showing similarities to eukaryotic multidrug transporters and downstream by a glycerol 3-phosphate cytidyltransferase (tagD)-like open reading frame presumably involved in teichoic acid synthesis. The abcA-pbp4-tagD gene cluster was located in the SmaI-D fragment in the S. aureus 8325 chromosome in close proximity to the RNA polymerase gene rpoB.
       }
}
@article{Mowbray_2025, title={A Survey of Deep Learning Architectures in Modern Machine Learning Systems: From CNNs to Transformers}, volume={4}, url={https://ashpress.org/index.php/jcts/article/view/204}, DOI={10.5281/zenodo.17035434}, abstractNote={&amp;lt;p&amp;gt;Deep learning has become a cornerstone of modern machine learning systems, empowering breakthroughs across domains such as computer vision, natural language processing, speech recognition, and autonomous control. This survey provides a comprehensive overview of the evolution, design principles, and application of deep learning architectures, with a particular focus on Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and Transformer-based models. We begin by tracing the historical development of neural architectures, highlighting the shift from spatial and temporal encoders to attention-driven models that enable long-range dependency modeling and cross-modal learning. We then present a detailed analysis of architectural components, including convolutional layers, recurrent units, self-attention mechanisms, normalization techniques, and position encoding strategies, emphasizing their mathematical foundations and design trade-offs. Furthermore, we explore the deployment of these architectures in diverse domains, illustrating real-world use cases and performance comparisons through visual diagrams. The survey also identifies major challenges in current deep learning systems-such as interpretability, data efficiency, scalability, and ethical deployment-and outlines promising directions including federated learning, parameter-efficient fine-tuning, biologically inspired computation, and unified multimodal frameworks. By synthesizing the architectural trajectory from CNNs to Transformers, this survey aims to guide researchers and practitioners in selecting, adapting, and advancing deep learning models to meet the evolving demands of real-world machine learning applications. Our findings highlight both the robustness and limitations of current approaches, offering insights into the next generation of intelligent and adaptable systems.&amp;lt;/p&amp;gt;}, number={8}, journal={Journal of Computer Technology and Software}, author={Mowbray, Thayer}, year={2025}, month={Aug.} }